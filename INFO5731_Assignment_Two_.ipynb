{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jivitheshreddy/INFO-5731-Srping2023/blob/main/INFO5731_Assignment_Two_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5617090f",
      "metadata": {
        "id": "5617090f"
      },
      "source": [
        "**Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2489608",
      "metadata": {
        "id": "e2489608"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using a hashtag (you can use any hashtag) from Twitter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21686f7f",
      "metadata": {
        "id": "21686f7f"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "import numpy as np\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "url=\"https://www.imdb.com/title/tt10640346/reviews?ref_=tt_sa_3\"\n",
        "import pandas as pd\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057df5ef",
      "metadata": {
        "id": "057df5ef"
      },
      "outputs": [],
      "source": [
        "browser=webdriver.Firefox()\n",
        "browser.get(url)\n",
        "html=browser.page_source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f416c8",
      "metadata": {
        "id": "79f416c8"
      },
      "outputs": [],
      "source": [
        "soup=BeautifulSoup(html,'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df039345",
      "metadata": {
        "id": "df039345"
      },
      "outputs": [],
      "source": [
        "base_url=\"https://www.imdb.com/title/tt10640346/reviews?ref_=tt_sa_3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465a7d35",
      "metadata": {
        "id": "465a7d35",
        "outputId": "fc0942ad-b06c-4c7b-f28b-1de79bba50d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [09:13<00:00,  1.81it/s]\n"
          ]
        }
      ],
      "source": [
        "master = {}\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a function to scrape all reviews from a page, including \"load more\" option\n",
        "def scrape_reviews(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
        "    review = []\n",
        "    count = 0\n",
        "    while True:\n",
        "        resp = requests.get(url, headers=headers).text\n",
        "        soup = BeautifulSoup(resp, 'html.parser')\n",
        "        for i in soup.find_all(\"div\", class_=\"content\"):\n",
        "            review.append(i.text.strip())\n",
        "            count += 1\n",
        "            if count == 10000:\n",
        "                break\n",
        "        if count == 10000:\n",
        "            break\n",
        "        next_link = soup.find(\"a\", class_=\"page next\")\n",
        "        if next_link:\n",
        "            url = next_link['href']\n",
        "        else:\n",
        "            break\n",
        "    table = pd.DataFrame({'Review': review})\n",
        "    return table\n",
        "\n",
        "# Loop through all pages and scrape reviews\n",
        "for i in tqdm(range(1, 1001)):\n",
        "    url = base_url.format(i)\n",
        "    master[i] = scrape_reviews(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6db6d63",
      "metadata": {
        "id": "f6db6d63"
      },
      "outputs": [],
      "source": [
        "big_table=pd.concat([master[i] for i in master],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8a10c5",
      "metadata": {
        "id": "db8a10c5"
      },
      "outputs": [],
      "source": [
        "big_table=big_table.head(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edda0563",
      "metadata": {
        "id": "edda0563"
      },
      "source": [
        " **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b55d567c",
      "metadata": {
        "id": "b55d567c"
      },
      "source": [
        "\n",
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b6af2d",
      "metadata": {
        "id": "24b6af2d",
        "outputId": "1babd24c-4c08-461b-9d8c-56051a7060ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jivit\\AppData\\Local\\Temp\\ipykernel_20432\\14359710.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  big_table['Review'] = big_table['Review'].str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0    Whether it be orgies showcasing various bodily...\n",
              "1    So many reviews praise this film for the level...\n",
              "2    After an interesting opening scene about our m...\n",
              "3    This film felt like it was written and directe...\n",
              "4    In the opening scene of BABYLON an elephant on...\n",
              "Name: Review, dtype: object"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Removing Numbers and Puncuation\n",
        "big_table['Review'] = big_table['Review'].str.replace('[^\\w\\s]','')\n",
        "big_table['Review'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d450cc0",
      "metadata": {
        "id": "4d450cc0",
        "outputId": "34a1b8ef-f04f-4df2-9940-3b4052800142"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Whether orgies showcasing various bodily fluid...\n",
              "1    So many reviews praise film level debauchery s...\n",
              "2    After interesting opening scene main character...\n",
              "3    This film felt like written directed high scho...\n",
              "4    In opening scene BABYLON elephant pickup empti...\n",
              "Name: Review, dtype: object"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "big_table['Review'] = big_table['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "big_table['Review'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e78037",
      "metadata": {
        "id": "27e78037",
        "outputId": "59f63e21-c7ba-4557-f61e-1076977efc2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    whether orgies showcasing various bodily fluid...\n",
              "1    so many reviews praise film level debauchery s...\n",
              "2    after interesting opening scene main character...\n",
              "3    this film felt like written directed high scho...\n",
              "4    in opening scene babylon elephant pickup empti...\n",
              "Name: Review, dtype: object"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "big_table['Review'] = big_table['Review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "big_table['Review'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c783e7b6",
      "metadata": {
        "id": "c783e7b6",
        "outputId": "895c0e07-8b49-48bb-cbb6-562ee45cf5b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\jivit\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\jivit\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               Review  \\\n",
            "0   whether orgies showcasing various bodily fluid...   \n",
            "1   so many reviews praise film level debauchery s...   \n",
            "2   after interesting opening scene main character...   \n",
            "3   this film felt like written directed high scho...   \n",
            "4   in opening scene babylon elephant pickup empti...   \n",
            "..                                                ...   \n",
            "20  score 2010my brain hurts wow awful i wouldnt e...   \n",
            "21  damian chazelle doesnt make bad movies whiplas...   \n",
            "22  damien chazelle chosen tag along filmmakers de...   \n",
            "23  a series lengthy chaotic scenes put together s...   \n",
            "24  i really want understand producers thought who...   \n",
            "\n",
            "                                         stemmed_text  \\\n",
            "0   whether orgi showcas variou bodili fluid plot ...   \n",
            "1   so mani review prais film level debaucheri sho...   \n",
            "2   after interest open scene main charact manni d...   \n",
            "3   thi film felt like written direct high school ...   \n",
            "4   in open scene babylon eleph pickup empti bowel...   \n",
            "..                                                ...   \n",
            "20  score 2010mi brain hurt wow aw i wouldnt even ...   \n",
            "21  damian chazel doesnt make bad movi whiplash la...   \n",
            "22  damien chazel chosen tag along filmmak decid v...   \n",
            "23  a seri lengthi chaotic scene put togeth substa...   \n",
            "24  i realli want understand produc thought who au...   \n",
            "\n",
            "                                      lemmatized_text  \n",
            "0   whether orgy showcasing various bodily fluid p...  \n",
            "1   so many review praise film level debauchery sh...  \n",
            "2   after interesting opening scene main character...  \n",
            "3   this film felt like written directed high scho...  \n",
            "4   in opening scene babylon elephant pickup empty...  \n",
            "..                                                ...  \n",
            "20  score 2010my brain hurt wow awful i wouldnt ev...  \n",
            "21  damian chazelle doesnt make bad movie whiplash...  \n",
            "22  damien chazelle chosen tag along filmmaker dec...  \n",
            "23  a series lengthy chaotic scene put together su...  \n",
            "24  i really want understand producer thought who ...  \n",
            "\n",
            "[10000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Define the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to apply the stemmer and lemmatizer to each word in a string\n",
        "def stem_and_lemmatize(text):\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Stem and lemmatize each word\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # Join the stemmed and lemmatized words back into a string\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    lemmatized_text = ' '.join(lemmatized_words)\n",
        "    # Return the stemmed and lemmatized texts as a tuple\n",
        "    return (stemmed_text, lemmatized_text)\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "big_table[['stemmed_text', 'lemmatized_text']] = big_table['Review'].apply(stem_and_lemmatize).apply(pd.Series)\n",
        "\n",
        "# Print the result\n",
        "print(big_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed27ed75",
      "metadata": {
        "id": "ed27ed75",
        "outputId": "c18e7473-76ee-4fec-899c-032b172602d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>stemmed_text</th>\n",
              "      <th>lemmatized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>whether orgies showcasing various bodily fluid...</td>\n",
              "      <td>whether orgi showcas variou bodili fluid plot ...</td>\n",
              "      <td>whether orgy showcasing various bodily fluid p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>so many reviews praise film level debauchery s...</td>\n",
              "      <td>so mani review prais film level debaucheri sho...</td>\n",
              "      <td>so many review praise film level debauchery sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>after interesting opening scene main character...</td>\n",
              "      <td>after interest open scene main charact manni d...</td>\n",
              "      <td>after interesting opening scene main character...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this film felt like written directed high scho...</td>\n",
              "      <td>thi film felt like written direct high school ...</td>\n",
              "      <td>this film felt like written directed high scho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in opening scene babylon elephant pickup empti...</td>\n",
              "      <td>in open scene babylon eleph pickup empti bowel...</td>\n",
              "      <td>in opening scene babylon elephant pickup empty...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>score 2010my brain hurts wow awful i wouldnt e...</td>\n",
              "      <td>score 2010mi brain hurt wow aw i wouldnt even ...</td>\n",
              "      <td>score 2010my brain hurt wow awful i wouldnt ev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>damian chazelle doesnt make bad movies whiplas...</td>\n",
              "      <td>damian chazel doesnt make bad movi whiplash la...</td>\n",
              "      <td>damian chazelle doesnt make bad movie whiplash...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>damien chazelle chosen tag along filmmakers de...</td>\n",
              "      <td>damien chazel chosen tag along filmmak decid v...</td>\n",
              "      <td>damien chazelle chosen tag along filmmaker dec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>a series lengthy chaotic scenes put together s...</td>\n",
              "      <td>a seri lengthi chaotic scene put togeth substa...</td>\n",
              "      <td>a series lengthy chaotic scene put together su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>i really want understand producers thought who...</td>\n",
              "      <td>i realli want understand produc thought who au...</td>\n",
              "      <td>i really want understand producer thought who ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Review  \\\n",
              "0   whether orgies showcasing various bodily fluid...   \n",
              "1   so many reviews praise film level debauchery s...   \n",
              "2   after interesting opening scene main character...   \n",
              "3   this film felt like written directed high scho...   \n",
              "4   in opening scene babylon elephant pickup empti...   \n",
              "..                                                ...   \n",
              "20  score 2010my brain hurts wow awful i wouldnt e...   \n",
              "21  damian chazelle doesnt make bad movies whiplas...   \n",
              "22  damien chazelle chosen tag along filmmakers de...   \n",
              "23  a series lengthy chaotic scenes put together s...   \n",
              "24  i really want understand producers thought who...   \n",
              "\n",
              "                                         stemmed_text  \\\n",
              "0   whether orgi showcas variou bodili fluid plot ...   \n",
              "1   so mani review prais film level debaucheri sho...   \n",
              "2   after interest open scene main charact manni d...   \n",
              "3   thi film felt like written direct high school ...   \n",
              "4   in open scene babylon eleph pickup empti bowel...   \n",
              "..                                                ...   \n",
              "20  score 2010mi brain hurt wow aw i wouldnt even ...   \n",
              "21  damian chazel doesnt make bad movi whiplash la...   \n",
              "22  damien chazel chosen tag along filmmak decid v...   \n",
              "23  a seri lengthi chaotic scene put togeth substa...   \n",
              "24  i realli want understand produc thought who au...   \n",
              "\n",
              "                                      lemmatized_text  \n",
              "0   whether orgy showcasing various bodily fluid p...  \n",
              "1   so many review praise film level debauchery sh...  \n",
              "2   after interesting opening scene main character...  \n",
              "3   this film felt like written directed high scho...  \n",
              "4   in opening scene babylon elephant pickup empty...  \n",
              "..                                                ...  \n",
              "20  score 2010my brain hurt wow awful i wouldnt ev...  \n",
              "21  damian chazelle doesnt make bad movie whiplash...  \n",
              "22  damien chazelle chosen tag along filmmaker dec...  \n",
              "23  a series lengthy chaotic scene put together su...  \n",
              "24  i really want understand producer thought who ...  \n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "big_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f5a0b8",
      "metadata": {
        "id": "f9f5a0b8",
        "outputId": "4f0cd641-6062-4d68-f6d3-b96e860b05b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\jivit\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\jivit\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\jivit\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tree import Tree\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def analyze_text(df):\n",
        "    # Combine all reviews into a single string\n",
        "    text = ' '.join(big_table['lemmatized_text'])\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # POS tagging\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    noun_count = len([word for word, pos in pos_tags if pos.startswith('N')])\n",
        "    verb_count = len([word for word, pos in pos_tags if pos.startswith('V')])\n",
        "    adj_count = len([word for word, pos in pos_tags if pos.startswith('J')])\n",
        "    adv_count = len([word for word, pos in pos_tags if pos.startswith('R')])\n",
        "\n",
        "    # Constituency parsing and dependency parsing\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        # Constituency parsing\n",
        "        tree = nltk.parse.stanford.StanfordParser().parse(nltk.word_tokenize(sentence))\n",
        "        for subtree in tree.subtrees():\n",
        "            if subtree.label() == 'NP':\n",
        "                print(subtree)\n",
        "\n",
        "        # Dependency parsing\n",
        "        dep_parser = nltk.parse.stanford.StanfordDependencyParser()\n",
        "        result = dep_parser.raw_parse(sentence)\n",
        "        dep = result.__next__()\n",
        "        for triple in dep.triples():\n",
        "            print(triple)\n",
        "\n",
        "    # Named entity recognition\n",
        "    ne_tree = ne_chunk(pos_tags)\n",
        "    named_entities = {}\n",
        "    for subtree in ne_tree.subtrees():\n",
        "        if subtree.label() == 'PERSON':\n",
        "            entity = ' '.join([word for word, pos in subtree.leaves()])\n",
        "            named_entities[entity] = named_entities.get(entity, 0) + 1\n",
        "        elif subtree.label() == 'ORGANIZATION':\n",
        "            entity = ' '.join([word for word, pos in subtree.leaves()])\n",
        "            named_entities[entity] = named_entities.get(entity, 0) + 1\n",
        "        elif subtree.label() == 'GPE':\n",
        "            entity = ' '.join([word for word, pos in subtree.leaves()])\n",
        "            named_entities[entity] = named_entities.get(entity, 0) + 1\n",
        "        elif subtree.label() == 'DATE':\n",
        "            entity = ' '.join([word for word, pos in subtree.leaves()])\n",
        "            named_entities[entity] = named_entities.get(entity, 0) + 1\n",
        "\n",
        "    return noun_count, verb_count, adj_count, adv_count, named_entities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5fc4f1",
      "metadata": {
        "id": "3c5fc4f1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['STANFORD_PARSER'] = 'C:/stanford-parser/stanford-parser.jar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7026c3c",
      "metadata": {
        "id": "d7026c3c"
      },
      "outputs": [],
      "source": [
        "noun_count, verb_count, adj_count, adv_count, named_entities = analyze_text(big_table['lemmatized_text'])\n",
        "os.environ['STANFORD_MODELS'] = 'C:/stanford-parser/stanford-parser-3.9.2-models.jar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89d351d",
      "metadata": {
        "id": "c89d351d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f519428",
      "metadata": {
        "id": "3f519428"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['STANFORD_PARSER'] = 'C:/stanford-parser/stanford-parser.jar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9736b3",
      "metadata": {
        "id": "4c9736b3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}